# ğŸ› ï¸ Architecture and Local Code Retrieval

> ğŸ” *An on-premise, privacy-preserving architecture for context-aware LLM-powered code assistance.*

---

## ğŸ“¦ System Overview
Our system comprises:

* ğŸŒ IDE plugin (JetBrains / VSCode)
* ğŸ§  On-premise LLM backend
* ğŸ” Local Personal CodeRAG module
* ğŸ©µ Logging & audit service

All code stays local. Only minimal metadata is shared.

### ğŸ”¹ Context-Level Architecture (C4)

![Context Level Diagram](../figures/Context_lvl.pdf)

> Developers interact with the plugin, which securely interfaces with internal LLMs and retrieval modules.

---

### ğŸ”¹ Container-Level Architecture (C4)

![Container Level Diagram](../figures/Container_lvl.pdf)

> Each container is modular and easily swappable: VSCode/JetBrains plugin, Admin UI, Ollama-based LLM backend, logging DB, and more.

---

## ğŸ” Personal CodeRAG: Local Retrieval Engine

This client-side component ensures:

* **Full code privacy**
* **Fast semantic retrieval**
* **Rich context for LLM prompting**

### ğŸ§  Embedding Pipeline

* `BERT (fine-tuned)` â€” encodes both queries and code snippets
* `FAISS` â€” performs similarity search using embeddings
* `GGUF` â€” compresses the model under 2GB
* `Ollama` â€” lightweight inference server

### âš™ï¸ Local Workflow

```
1. Developer writes a prompt in IDE
2. CodeRAG embeds the query and searches local indexed code
3. Top-K relevant snippets are returned
4. Context is passed to LLM (on-prem/cloud-hosted) for generation
```

### âœ¨ Key Benefits

* ğŸ”’ No code leaves the developerâ€™s machine
* ğŸ§¹ Plug-and-play for different IDEs or LLM backends
* ğŸ’» Works on low-resource environments (<2GB RAM)
* âš™ï¸ Fully modular and extensible

---

## âš™ï¸ Technologies Used

| Component          | Tool/Framework           |
| ------------------ | ------------------------ |
| IDE Plugin         | JetBrains / VSCode       |
| Retrieval Index    | FAISS                    |
| Embeddings         | BERT (Code-tuned)        |
| Model format       | GGUF                     |
| Orchestration      | Ollama                   |
| Hosting (optional) | Docker, Kubernetes, etc. |

---

## ğŸ§  Learn More

* [C4 Model](https://c4model.com/)
* [GGUF Format](https://github.com/ggerganov/ggml)
* [Ollama](https://ollama.com/)
* [FAISS](https://github.com/facebookresearch/faiss)

